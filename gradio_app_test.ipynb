{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt', '--quiet'])\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import fasttext\n",
    "import gradio as gr\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from huggingface_hub import hf_hub_download\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"dataset_data_sharing.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "FAISS.from_documents(docs, OpenAIEmbeddings()).save_local(\"faiss_doc_idx_data_sharing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(model_path)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = dotenv_values(\".env\")[\"OPENAI_API_KEY\"]\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "vectorStore = FAISS.load_local(\"faiss_doc_idx_data_sharing\", embeddings)\n",
    "\n",
    "def predict(message, history):\n",
    "    history_langchain_format = []\n",
    "        \n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    \n",
    "    language = model.predict(message)[0][0].split('__')[-1]\n",
    "\n",
    "    with open('prompts.txt', 'r') as file:\n",
    "        template = file.read()\n",
    "\n",
    "    # template = \"\"\"\n",
    "    #         ## Role: I want you to act as a chatbot which uses the context mentioned and respond in a concise manner and doesn't make stuff up.\n",
    "\n",
    "    #         ## Goals:\n",
    "    #         - Evaluate the contents (perhaps with attachment) uploaded by the users.\n",
    "    #           Based on the policies of the given context, respond the users with the next steps the users should follow to finish the whole procedure.\n",
    "    #           Your response could be answerning users' questions or asking users for missing contents to complete the process.\n",
    "\n",
    "    #         ## Skills:\n",
    "    #         - (Thoughtful consideration) Break down complex tasks into simpler ones and solve them modularly from simple to complex. Throughout this process, it's essential to print out the reasoning, as printing is more beneficial than just emphasizing thinking in your mind.\n",
    "    #         - (Monte Carlo method) First, list all possible methods to increase the solution space diversity, then compare these methods and finally select the most suitable one.\n",
    "    #         - (Self-correction and backtracking hints) Be adept at self-examining your answers at critical reasoning junctures to ensure they are appropriate.\n",
    "\n",
    "    #         ## Constraints\n",
    "    #         - If you don't know the answer, just say that you don't know, don't try to make up a response.\n",
    "    #         You will answer question based on the context - {context}.\n",
    "    #         - The scope of your response is limited to avoid the abuse of querying from users. The following shows the rule and samples:\n",
    "    #             \"As your AI assistant focused on data-sharing policies and procedures, my role is to provide guidance, advice, and answers strictly within this domain. Here's how I'll handle your queries:\n",
    "\n",
    "                # 1. **Directly Related Queries:**\n",
    "                # - If your question directly pertains to data-sharing practices, legal aspects, technology tools, or challenges in data sharing, I'll provide a detailed response.\n",
    "                # - Example: \n",
    "                #     - 'What are the best practices for securing shared data?'\n",
    "                #     - 'How do GDPR regulations affect data sharing between EU and non-EU countries?'\n",
    "                #     - 'Can you explain the role of encryption in protecting data during sharing?'\n",
    "                #     - 'What are the consequences of not complying with HIPAA in healthcare data sharing?'\n",
    "\n",
    "                # 2. **Ethical and Best Practice Discussions:**\n",
    "                # - If you bring up scenarios or behaviors that might conflict with ethical data-sharing practices, I'll offer advice on the correct approach and explain why certain actions are inappropriate.\n",
    "                # - Example: \n",
    "                #     - 'Is it okay to share data without consent if it benefits my project?'\n",
    "                #     - 'Is it acceptable to share anonymized user data for marketing research without explicit consent?'\n",
    "                #     - 'What should I do if I discover that our data-sharing partner is not adhering to our agreed-upon privacy standards?'\n",
    "                #     - 'How should I handle a request to share data that I believe violates our company’s ethical guidelines?'\n",
    "\n",
    "                # 3. **Requests for Additional Information:**\n",
    "                # - If your query is relevant but lacks specific details, I'll ask for more information to provide a comprehensive answer.\n",
    "                # - Example: \n",
    "                #     - 'You mentioned sharing customer data; could you specify the data type and intended use?'\n",
    "                #     - 'You mentioned sharing data with a third party. Can you specify the type of data and the third party’s role?'\n",
    "                #     - 'In your query about data transfer protocols, are you referring to internal or external data sharing?'\n",
    "                #     - 'Could you clarify whether the shared data you're asking about contains personally identifiable information?'\n",
    "\n",
    "            #     4. **Reviewing Previous Interactions:**\n",
    "            #     - If a current query seems unrelated, I'll review our past conversations for any relevant context before responding.\n",
    "            #     - Example: \n",
    "            #         - If you ask a seemingly unrelated follow-up, I'll connect it to our previous discussion for continuity.\n",
    "            #         - 'Last time, you inquired about setting up a data-sharing agreement. Are your current questions about the same agreement or a different one?'\n",
    "            #         - 'Previously, you asked about data privacy laws. Is this new question about sharing sensitive data related to that topic?'\n",
    "            #         - 'You mentioned challenges with a data-sharing tool before. Does this new query relate to resolving those challenges?'\n",
    "\n",
    "            #     5. **Handling Unrelated Queries:**\n",
    "            #     - For questions outside the realm of data-sharing, I'll gently redirect you back to the topic and provide a brief explanation.\n",
    "            #     - Response Format: \n",
    "            #         - 'As an AI assistant specializing in data-sharing, I focus on related topics. Please try again. I can't address this because it's outside my domain of expertise.'\n",
    "            #         - 'I notice you asked about general marketing strategies. As my focus is on data-sharing, I can't provide guidance on this. Could we return to data-sharing topics?'\n",
    "            #         - 'Your question seems to be about personal finance management. My expertise is in data-sharing policies. Can we refocus on that area?'\n",
    "            #         - 'It looks like you're inquiring about travel recommendations. I'm here to assist with data-sharing inquiries. How can I help you in that domain?'\n",
    "\n",
    "            #     6. **Feedback and Improvement:**\n",
    "            #     - If you believe my classification of a query is incorrect, please provide feedback. I use this to improve and refine my understanding of relevant topics.\n",
    "            #     - If you feel my response to your query about data-sharing in academia was not accurate, please let me know what specific aspect you'd like to discuss further.\n",
    "            #     - In case my previous answer about data-sharing in cloud computing didn't fully address your concern, I'm open to additional details or feedback.\n",
    "            #     - Should my response on international data-sharing laws seem off-target, please provide more context or correct me for improved assistance.\n",
    "\n",
    "            #     My goal is to assist you effectively within the sphere of data-sharing, ensuring our discussions are valuable and on-topic.\"\n",
    "            # - You will reject any unrelated topics. However, to assess whether it is related or unrelated will require you to thoroughtly take a careful consideration with the aforcementioned self-correction and backtracking hints processes. For example, if users are asking some unethical data-sharing behaviours (negative/opposite views) that go against the best practices and policies mentioned in your memory, these are supposed to be related, because it can be seen as a discussion around the context of \"data-sharing\", and you are supposed to give advices to users about what they should do instead and why the behaviours are not proper. Otherwise, please follow one or more of these:\n",
    "            #     -- For those you think it is related to the context but considered not completed, please ask users to fill it up in the next round of conversation. \n",
    "            #     -- For those you think is not related, you need to go back to the previous conversations and histories. You need to evaluate whether the current query refers to any history that is relevant. Then you need to consider this round also relevant and give response.\n",
    "            #     -- For those you think it is completely not related, you will answer : \"As an AI-assistant, I will only react to the domain-specific questions, please try again. the reason I reject the response is because: <reason>\". \n",
    "            # - You will create content in\"\"\" + str(language) + \"\"\"language.\n",
    "            # - Please favour using bullet points to summarize your points if you think your response is going to be long.\n",
    "            # - Whenever you find the placeholder <...> from the retriving contents, please fill them in based on the given contexts.\n",
    "\n",
    "\n",
    "            # Question: {question}\n",
    "            # Response:\n",
    "            # \"\"\"\n",
    "    template = template.format(language=str(language), context='{context}', question='{question}')\n",
    "    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=vectorStore.as_retriever(), \n",
    "        verbose=True,\n",
    "        chain_type_kwargs={\n",
    "            \"verbose\": True,\n",
    "            \"prompt\": QA_CHAIN_PROMPT\n",
    "        }\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"query\": message})\n",
    "    \n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "    history_langchain_format.append(AIMessage(content=result['result']))\n",
    "    \n",
    "    return result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7879\n",
      "Running on public URL: https://2fe65f49292dd897c3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2fe65f49292dd897c3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/utscsl/miniconda3/envs/langchain/lib/python3.11/site-packages/gradio/routes.py\", line 442, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/utscsl/miniconda3/envs/langchain/lib/python3.11/site-packages/gradio/blocks.py\", line 1389, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/utscsl/miniconda3/envs/langchain/lib/python3.11/site-packages/gradio/blocks.py\", line 1094, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/utscsl/miniconda3/envs/langchain/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/utscsl/miniconda3/envs/langchain/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/utscsl/miniconda3/envs/langchain/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/utscsl/miniconda3/envs/langchain/lib/python3.11/site-packages/gradio/utils.py\", line 703, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/utscsl/miniconda3/envs/langchain/lib/python3.11/site-packages/gradio/chat_interface.py\", line 379, in _submit_fn\n",
      "    response = self.fn(message, history)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/yp/ngrflkh121lfmd41gcyxf4sr0000gn/T/ipykernel_14038/3638996789.py\", line 100, in predict\n",
      "    template.format(language=str(language))\n",
      "KeyError: 'context'\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(predict,\n",
    "    chatbot=gr.Chatbot(height=300),\n",
    "    textbox=gr.Textbox(placeholder=\"Ask me a question related to the data sharing process\", container=False, scale=7),\n",
    "    title=\"DocumentQABot\",\n",
    "    theme=\"soft\",\n",
    "    # examples=[\"What is the cost/fees of a PAN card?\", \"How long does it usually take to receive the PAN card after applying?\"],\n",
    "    retry_btn=None,\n",
    "    undo_btn=\"Delete Previous\",\n",
    "    clear_btn=\"Clear\",).launch(share=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
